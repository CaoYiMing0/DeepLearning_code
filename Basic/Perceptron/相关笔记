特此说明：以下涉及到的公式格式或许有问题
感知器前馈计算过程：
    y = f(w·x + b) = f(w₁x₁ + w₂x₂ + w₃x₃ + bias)
step function(阶跃函数)：
    f(x) = 1  ,   x>0
           0  ,   x≤0
步骤：
    1、把输入项和权重相乘
    2、把所有乘积的结果累加
    3、把累加结果放进阶跃函数
接收多个端子的输入（有不同权重），在内部处理后产生一个输出，然后可再作为端子
    权重的数量决定输入的数量
偏置的作用：
    希望得到的直线不仅仅只能过原点，二是可以遍布坐标轴上的每一点


前馈计算的进一步说明：
    第一阶段：
        w和x相乘
    第二阶段：
        将结果累加起来z= w0·x0 + w1·x1 +…+ wn·xn
        一般情况下w0 = b(bias,偏置),x0 = 1
        可写成w = [w0, w1, w2,…, wn], x = [x0, x1, x2,…,xn]
        则z = w · x
    第三阶段：
        output = f(z)


感知器进阶——实现逻辑运算
    对于此理解可以把权重和偏置放在坐标系中来理解
    但仅能做0-1输出，仅能处理线性分类问题，无法处理线性分类问题（这也可以在坐标系中来理解）
    如何解决呢？
        多层感知器（神经网络的前身）


感知器再进阶——感知机（神经网络Neural Network的前身）
    结构：
        输入层——隐层——输出层
        上面这种只含一层隐层的感知机叫做单隐层感知机或双层（隐层和输出层）感知机
    隐层的定义:
        除输入层和输出层以外的其他各层都叫做隐层
        隐层不会直接给出输出信号
    隐层的输出计算：
        假定：
            所有同一隐层的神经元都与上一层的每个输出相连
            同一隐层的神经元之间不互相连接
        规定：
            wij(l)为第l层第i个神经元与第j个输入相连的权重
            第l层中每个神经元的权重为k维向量，k为（l-1）层神经元数量
            第l个隐层中所有p个神经元的权重组成p*k矩阵W(l)
        第l层神经元的输出为y(l) = output(l) = f(W(l)·output(l-1) + b)   偏置量b是一个矩阵


隐层的作用
    结构      决策区域类型
    无隐层     由一超平面分成两个
    单隐层     开凸区域或闭凸区域
    双隐层     任意形状（其复杂度有单元数目决定）

