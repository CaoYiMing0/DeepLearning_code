机器学习缺乏对非结构化数据的提取能力
自动特征提取：
    深度学习与传统模式识别方法最大的不同在于它能够自动提取特征，机器学习中的特征提取往往具有很强的针对性
深度学习在计算机视觉上的细分领域：
    图像处理（Image Processing）
    图片分类（Image Classification）
    模式识别（Pattern Recognition）
    图像理解（Image Understanding）
    图像分割（Image Segmentation）
    图像检测（Image Detection）


深度学习的标准架构
    数据
        预处理
    网络
        标准处理单元
            神经网络
            Batch Normalization
            Dropout
        激活函数
        连接方式
            全连接方式
            卷积连接方式
            循环连接方式
        前向传播
        输出层与ground truth
    训练
        目标函数
        学习率
        优化算法
            梯度下降法
            更新算法
        反向传播
    评估与调参
        过拟合/欠拟合
        正则化
        初始化
        迁移学习与蒸馏法


名词解释：
    • 损失(loss)：衡量神经网络的输出和ground truth有多大差异的值
    • 损失函数(loss function)：计算损失的函数，又称为代价函数，误差函数
    • 梯度(gradient)：损失函数对权重的导数
    • 梯度下降(gradient descent)：根据梯度来更新权重，使损失变小的方法
    • 训练(training)：使用梯度下降更新w，直到满足模型终止要求的过程
    • 训练step：一次"计算梯度，更新权重"的过程
    • 数据集(dataset)：用来训练神经网络的数据，它包括输入数据和ground truth
    • epoch：整个数据集中每条数据都训练过一次，叫一个epoch
    • 梯度函数：损失函数的导函数


数据预处理
    0均值
    归一化
    主成分分析（PCA）
    白化：在PCA基础上，对数据进行标准差归一化处理


PCA计算步骤
    0均值处理
    计算协方差矩阵
        求协方差矩阵及其特征值，找到数据分散程度最大的方向
    计算特征值和特征向量
        求特征值对应的特征向量，通过特征向量实现坐标转换
    选择主成分
        只保留数据分散程度较大的维度
    降低数据维度


PCA分析和白化涉及到大的计算量，适合数据量较小的数据，项目中用不用要根据项目实际情况来定


各种激活函数的优缺点
    sigmoid:非线性分界，两端软饱和，输出为区间(0,1)
        优点：
            相比于unit(x)函数，增加了非线性表达能力
        缺点：
            1.有软饱和去，存在梯度消失问题
            2.输出恒为正，可能存在zigzag现象
    tanh(x)：非线性分界，两端软饱和，输出区间(-1,1)
        优点：
            函数输出以0为中心，没有zigzag现象
        缺点：
            有软饱和区，仍存在梯度消失问题
    ReLU(x)：非线性分界，左侧硬饱和，右侧无饱和，输出为区间[0,+∞)
        优点：
            1.右侧梯度保持不变，改善了梯度消失问题
            2.只有线性关系，不用指数运算，因此计算速度快
        缺点：
            1.左侧梯度为零，导致神经元不再更新（神经元死亡）
            2.输出为负，仍存在zigzag线性

一些新的激活函数PReLU、ＭaxOut、ELU、SELU和Swish


                       深度学习训练与优化

参数的初始化
    权重的初始化：
        最原始的初实话：全部为固定值
            Wij = a
        稍好些的初始化：服从固定方差的独立高斯分布
            W ~ N(0 ,a²)
            随着层级的加深，高斯分布初始化的参数容易出现过大或过小的问题
            解决方法：Xavier初始化或MSRA初始化！
    Xavier初始化：服从参数为n的均匀分布或独立高斯分布
    MSRA初始化：服从参数为n的独立高斯分布(使用ReLU函数推导的 )


欠拟合和过拟合的解决办法
    欠拟合解决办法
        添加其他特征性
        添加多项式特征
        减少正则化参数
    过拟合解决办法
        扩大训练集数据
        提前终止（Early Stop）
            在模型训练的时候，在训练集上准确率一直在增长，到一定程度不增长
        数据集扩增（Data Augmentation）
            图片多样裁剪，图片上下左右翻转
        正则化
        Dropout


分类任务采用交叉熵来判定预测结果与期望输出的接近程度
    交叉熵见公式H(x)


ground truth:指监督学习中训练集的正确结果。也叫真值，用于指导模型训练。
    在监督学习中，数据会标注正确的输出，一般以(x, t)的形式出现，其中x是
    输入数据，t是ground truth。
    模型预测结果最后要与ground truth作比较，如果标注的不是ground
    truth，那么模型结果将严重偏离实际情况。
    例如给出一只狗的图片(“鹿”，０) (“狗”，1)(“人”，０)


神经网络的损失函数
目标函数＝损失函数＋正则项
        损失函数：使用训练出来的模型进行预测或者分类存在的误差的累积
        正则项：参数约束项

分类损失和回归损失
    分类损失：
        0-1损失
        指数损失
        合页损失
        对数损失（交叉熵）---常用
    回归损失：
        L1损失
        L2损失
        Huber损失（平滑L1）

改进损失函数：
    近些年人脸的发展集中在loss函数上
    • Focal loss
    • Center loss
    • triplet loss
    • contrasitve loss
    • Congenerous cosine loss/coco loss
    • large margin loss
    • Asoftmax loss
    • normface
    • AcrFace / insightface
    • AMsoftma



学习率
    梯度下降法和反向传播算法中的超参数η就是学习率，它控制着神经网络权值下降的速度
    公式θnew = θold  - η·gradθ

学习率衰减
    为什么要衰减？
        • 算法优化前期，学习率较大会加速学习，
        但后期会造成较大波动，出现围绕最优值 徘徊而无法收敛的情况，
        因此随着训练的 进行学习率需要逐渐衰减。
    什么时机衰减？
        • 通常是loss走平或震荡时
        • 或者一直衰减
    怎样衰减？
        • 1/10衰减
        • 1/3衰减
        • 0.94/0.87/0.74/0.575
        • 针对鞍点，采用循环学习率变化方式